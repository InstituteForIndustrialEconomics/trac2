% TRAC 2020 paper template for Ms8qQxMbnjJMgYcw
\documentclass[10pt, a4paper]{article}
\usepackage{lrec}
%\newcites{languageresource}{Language Resources}
\usepackage{graphicx}
\usepackage{tabularx}
\usepackage{soul}
% for eps graphics
%%% References and Labels
%%% Reference labels without a punctuation 
% courtesy of Marc Schulder , uni Hamburg ****************
\usepackage{titlesec}
\titleformat{\section}{\normalfont\large\bf\center}{\thesection.}{1em}{}
\titleformat{\subsection}{\normalfont\SmallTitleFont\bf\raggedright}{\thesubsection.}{1em}{}
\titleformat{\subsubsection}{\normalfont\normalsize\bf\raggedright}{\thesubsubsection.}{1em}{}
\renewcommand\thesection{\arabic{section}}
\renewcommand\thesubsection{\thesection.\arabic{subsection}}
\renewcommand\thesubsubsection{\thesubsection.\arabic{subsubsection}}
%  ed 

\usepackage{epstopdf}
\usepackage[utf8]{inputenc}

\usepackage{hyperref}
\usepackage{xstring}

\usepackage{color}

\newcommand{\secref}[1]{\StrSubstitute{\getrefnumber{#1}}{.}{ }}

\title{{BERT of all trades, master of some}}
% 
% one BERT to classify them all

\name{Author1, Author2, Author3}

\address{Affiliation1, Affiliation2, Affiliation3 \\
         Address1, Address2, Address3 \\
         author1@xxx.yy, author2@zzz.edu, author3@hhh.com\\
         \{author1, author5, author9\}@abc.org\\}


\abstract{
This paper describes our results for TRAC 2020 competition held together with the conference LREC 2020. Competition consisted of 2 subtasks in 3 languages each (Benghali, English and Hindi) where the participants' task was to classify aggression in short texts from social media and decide if it is gendered or not. We used a single BERT-based system with two outputs for all tasks simultaneously. Our model took the first place in English gendered text classification and the second place in Bengali gendered text classification.\\ \newline
\Keywords{aggression, classification, BERT, neural network, Transformer, NLP} }

\begin{document}

\maketitleabstract


\section{Introduction}
\label{intro}
This paper is devoted to our system's solution for TRAC 2020 competition held together with LREC 2020 conference. TRAC 2020 competition consisted of 2 sub-tasks in 3 languages: Bengali, English and Hindi. In the first sub-task contestants needed to make a system that labeled texts into three classes: ‘Overtly Aggressive’, ‘Covertly Aggressive’ and ‘Non-aggressive’. In the second task the contestants' task was to label the same texts as gendered or not. The dataset contained 18681 texts in total, approximately 6000 texts for each language.

We used a single BERT-based system with two outputs for all subtasks and languages simultaneously. Our model took the first place in English gendered text classification and the second place in Bengali gendered text classification.

\section{Related Work}
Aggression and misogyny detection is a rampant problem nowadays on the Internet. Many research initiatives have been devoted to its investigation. Given the overwhelming amount of information that social media users output every second, it is incomprehensible to monitor and moderate all of it manually. So it becomes useful to make at least semi-automatic predictions about whether a message contains aggression. Shared tasks and competitions are of great utility in this task because they provide data that can be used to research into new ways of aggression expression and allow different methods to be compared in a uniform and impartial way. Among such competitions we can name the previous TRAC competition \cite{trac2018report} and Offenseval \cite{offenseval}. The first TRAC shared task on aggression identification was devoted to a 3-way classification in between ‘Overtly Aggressive’, ‘Covertly Aggressive’ and ‘Non-aggressive’ Facebook text data in Hindi and English. Offenseval was very similar in nature but in contained texts only in English. It consisted of 3 subtasks: binary offense identification, binary categorization of offense types and offense target classification.

Private initiatives also do not keep out of this problem. For example, there were held several challenges on machine learning competition platform Kaggle devoted to aggression investigation in social media, among them: Jigsaw Toxic Comment Classification Challenge and Jigsaw Unintended Bias in Toxicity Classification.

Most researchers treat the problem of aggression detection as a classification problem. [POTAPOVA AND GORDEEV]. other mentions. The best solutions at Kaggle.

There are few competitions that have the data labelled in several languages at the same time. Thus, it might be troublesome to compare a single model for all languages to a special model dedicated to each language. However, [TRAC?]. Single model in machine translation by Google.
\section{TRAC-2 dataset}
TRAC 2020 competition contained 5000 texts in 3 languages: Bengali, English and Hindi.

The authors of the competition split texts in all languages into training, validation and test datasets.
\begin{table}[h]
	\begin{tabular}{|l|lll|}
		\hline
		\bf Dataset & \bf English & \bf Hindi & \bf Bengali\\ 
		\hline
		%%%%
		\hline
\bf Train & 	  4263 & 3984 & 3826\\  \hline
\bf Development & 1066 & 997  & 957\\  \hline
\bf Test & 		  1200 & 1200 & 1188\\ \hline\hline
\bf Total &       6529 & 6181 & 5971\\ \hline
	\end{tabular}
	%TODO : you should write a descriptive caption
	\caption{Number of texts for each language and dataset}
	\label{tab:dataset}
\end{table}

\section{BERT model with multiple outputs}
In this task we wanted to experiment with a single model that works with multiple languages at once. We could have used an embedding-based approach [Word2Vec, FastText]. However, pre-trained language models are usually trained for one language at a time.  Fortunately, it is possible to overcome this using multilingual language models such as BERT \cite{bert}.

BERT is a Transformer-based model \cite{attention}. It was trained using Wikipedia texts. All texts were tokenized using byte-pair encoding (BPE) which allows to limit the vocabulary size compared to Word2vec and other word vector models. The training consisted in predicting a random masked token in the sentence and the next sentence. We did not fine tune the language model. Information about the text language was not included into the model. The model had to infer it itself. We did not perform any text augmentation or pre-processing besides standard byte-pair encoding.
We used a multilingual uncased BERT model provided by Hugging Face \cite{Wolf2019HuggingFacesTS}. We used PyTorch framework to create our model.

Half precision training was used vi Apex library.

We used the same training, validation and test datasets as they were provided by the organizers.
\section{Results}
\label{sec:results}

\begin{table}[h]
\begin{tabular}{|llll|}
\hline
\bf Task & \bf F1 (weighted) & \bf Accuracy & \bf Rank\\ 
\hline
%%%%
\hline
Bengali-A & 0.7716 & 0.7811 & 4\\  \hline
Bengali-B & 0.9297 & 0.9293 & 2\\  \hline
English-A & 0.7568 & 0.7683 & 3\\ \hline
English-B & 0.8716 & 0.8708 & 1\\ \hline
Hindi-A & 0.7761 & 0.7683 & 4\\  \hline
Hindi-B & 0.8381 & 0.8392 & 3\\  \hline
\end{tabular}
%TODO : you should write a descriptive caption
\caption{Results for all tasks}
\label{tab:results}
\end{table}

\section{Conclusion}
This paper describes our results for TRAC 2020 competition held together with the conference LREC 2020. Competition consisted of 2 subtasks where participants had to classify aggression in texts and decide if it is gendered or not for 3 languages: Benghali, English and Hindi. We used a single BERT-based system with two outputs for all tasks simultaneously. Our model took the first place in English gendered text classification and the second place in Bengali gendered text classification.

\section{References}
\bibliography{trac2020}
\bibliographystyle{lrec}


\end{document}
