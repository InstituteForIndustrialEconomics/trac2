{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "trac2.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "3K4xb27MazQu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!git clone https://github.com/InstituteForIndustrialEconomics/trac2\n",
        "!pip install transformers\n",
        "!pip install seqeval\n",
        "!pip install --force https://github.com/chengs/tqdm/archive/colab.zip"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u2yExL8UZ48c",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%%writefile setup.sh\n",
        "\n",
        "git clone https://github.com/NVIDIA/apex\n",
        "cd apex\n",
        "pip install -v --no-cache-dir ./"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dq_qV9nea_7O",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!sh setup.sh"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ahcWw2r6Z9Dx",
        "colab_type": "code",
        "outputId": "2d76a098-e3b1-4d27-ad49-50cd4bf117df",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "cd trac2"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/trac2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M0vDyQb1K_DQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!git pull"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t9vZPdfJc4tR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ls"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tW2a4DLHM1oI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!python run_classification.py \\\n",
        "    --data_dir ./ \\\n",
        "    --model_type bert \\\n",
        "    --model_name_or_path bert-base-multilingual-uncased \\\n",
        "    --task_name trac \\\n",
        "    --output_dir trained-model \\\n",
        "    --max_seq_length 512 \\\n",
        "    --do_train \\\n",
        "    --do_eval \\\n",
        "    --evaluate_during_training \\\n",
        "    --per_gpu_train_batch_size 16 \\\n",
        "    --per_gpu_eval_batch_size 16 \\\n",
        "    --num_train_epochs 10 \\\n",
        "    --fp16 \\\n",
        "    --overwrite_output_dir"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JX3qU5E5u4Jk",
        "colab_type": "code",
        "outputId": "27eea6e2-bb3d-4381-cea9-6bf473e129c0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "! for folder in eng iben hin\n",
        "! do\n",
        "! python run_classification.py \\\n",
        "    --data_dir ./ \\\n",
        "    --model_type bert \\\n",
        "    --model_name_or_path bert-base-multilingual-uncased \\\n",
        "    --task_name trac \\\n",
        "    --output_dir trained-model \\\n",
        "    --max_seq_length 512 \\\n",
        "    --do_predict \\\n",
        "    --folder_list $folder \\\n",
        "    --evaluate_during_training \\\n",
        "    --per_gpu_train_batch_size 16 \\\n",
        "    --per_gpu_eval_batch_size 16 \\\n",
        "    --num_train_epochs 3 \\\n",
        "    --fp16 \\\n",
        "    --overwrite_output_dir\n",
        "! rm cached_test_bert-base-multilingual-uncased_512_trac\n",
        "! done"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "03/14/2020 11:16:29 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: True\n",
            "03/14/2020 11:16:29 - INFO - transformers.configuration_utils -   loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-uncased-config.json from cache at /root/.cache/torch/transformers/33b56ce0f312e47e4d77a57791a4fc6233ae4a560dd2bdd186107058294e58ab.c7892120c5a9b21e515abc904e398dbabddf9510b122f659063cbf361fe16868\n",
            "03/14/2020 11:16:29 - INFO - transformers.configuration_utils -   Model config BertConfig {\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"bos_token_id\": null,\n",
            "  \"directionality\": \"bidi\",\n",
            "  \"do_sample\": false,\n",
            "  \"eos_token_ids\": null,\n",
            "  \"finetuning_task\": \"trac\",\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\",\n",
            "    \"1\": \"LABEL_1\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"is_decoder\": false,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0,\n",
            "    \"LABEL_1\": 1\n",
            "  },\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"length_penalty\": 1.0,\n",
            "  \"max_length\": 20,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_beams\": 1,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"num_labels\": 2,\n",
            "  \"num_return_sequences\": 1,\n",
            "  \"output_attentions\": false,\n",
            "  \"output_hidden_states\": false,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": null,\n",
            "  \"pooler_fc_size\": 768,\n",
            "  \"pooler_num_attention_heads\": 12,\n",
            "  \"pooler_num_fc_layers\": 3,\n",
            "  \"pooler_size_per_head\": 128,\n",
            "  \"pooler_type\": \"first_token_transform\",\n",
            "  \"pruned_heads\": {},\n",
            "  \"repetition_penalty\": 1.0,\n",
            "  \"temperature\": 1.0,\n",
            "  \"top_k\": 50,\n",
            "  \"top_p\": 1.0,\n",
            "  \"torchscript\": false,\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_bfloat16\": false,\n",
            "  \"vocab_size\": 105879\n",
            "}\n",
            "\n",
            "03/14/2020 11:16:30 - INFO - transformers.tokenization_utils -   loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-uncased-vocab.txt from cache at /root/.cache/torch/transformers/bb773818882b0524dc53a1b31a2cc95bc489f000e7e19773ba07846011a6c711.535306b226c42cebebbc0dabc83b92ab11260e9919e21e2ab0beb301f267b4c7\n",
            "03/14/2020 11:16:30 - INFO - transformers.modeling_utils -   loading weights file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-uncased-pytorch_model.bin from cache at /root/.cache/torch/transformers/cc4042a0d6f70eae595ea0e6d49521b17bd6251f973b3e37d303ce7945b90eed.54b4dad9cc3db9aa8448458b782d11ab07c80dedb951906fd2f684a00ecdb1ee\n",
            "03/14/2020 11:16:35 - INFO - transformers.modeling_utils -   Weights of MultiHeadClassification not initialized from pretrained model: ['classifier_a.weight', 'classifier_a.bias', 'classifier_b.weight', 'classifier_b.bias']\n",
            "03/14/2020 11:16:35 - INFO - transformers.modeling_utils -   Weights from pretrained model not used in MultiHeadClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
            "03/14/2020 11:16:38 - INFO - __main__ -   Training/evaluation parameters Namespace(adam_epsilon=1e-08, cache_dir='', config_name='', data_dir='./', device=device(type='cuda'), do_eval=False, do_lower_case=False, do_predict=True, do_train=False, eval_all_checkpoints=False, evaluate_during_training=True, folder_list=['eng'], fp16=True, fp16_opt_level='O1', gradient_accumulation_steps=1, learning_rate=5e-05, local_rank=-1, logging_steps=500, max_grad_norm=1.0, max_seq_length=512, max_steps=-1, model_name_or_path='bert-base-multilingual-uncased', model_type='bert', n_gpu=1, no_cuda=False, num_train_epochs=3.0, output_dir='trained-model', output_mode='classification', overwrite_cache=False, overwrite_output_dir=True, per_gpu_eval_batch_size=16, per_gpu_train_batch_size=16, save_steps=500, seed=42, server_ip='', server_port='', task_name='trac', tokenizer_name='', warmup_steps=0, weight_decay=0.0)\n",
            "03/14/2020 11:16:38 - INFO - transformers.tokenization_utils -   Model name 'trained-model' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, bert-base-finnish-cased-v1, bert-base-finnish-uncased-v1, bert-base-dutch-cased). Assuming 'trained-model' is a path, a model identifier, or url to a directory containing tokenizer files.\n",
            "03/14/2020 11:16:38 - INFO - transformers.tokenization_utils -   Didn't find file trained-model/added_tokens.json. We won't load it.\n",
            "03/14/2020 11:16:38 - INFO - transformers.tokenization_utils -   loading file trained-model/vocab.txt\n",
            "03/14/2020 11:16:38 - INFO - transformers.tokenization_utils -   loading file None\n",
            "03/14/2020 11:16:38 - INFO - transformers.tokenization_utils -   loading file trained-model/special_tokens_map.json\n",
            "03/14/2020 11:16:38 - INFO - transformers.tokenization_utils -   loading file trained-model/tokenizer_config.json\n",
            "03/14/2020 11:16:38 - INFO - __main__ -   Evaluate the following checkpoints: ['trained-model']\n",
            "03/14/2020 11:16:38 - INFO - transformers.configuration_utils -   loading configuration file trained-model/config.json\n",
            "03/14/2020 11:16:38 - INFO - transformers.configuration_utils -   Model config BertConfig {\n",
            "  \"architectures\": [\n",
            "    \"MultiHeadClassification\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"bos_token_id\": null,\n",
            "  \"directionality\": \"bidi\",\n",
            "  \"do_sample\": false,\n",
            "  \"eos_token_ids\": null,\n",
            "  \"finetuning_task\": \"trac\",\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\",\n",
            "    \"1\": \"LABEL_1\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"is_decoder\": false,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0,\n",
            "    \"LABEL_1\": 1\n",
            "  },\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"length_penalty\": 1.0,\n",
            "  \"max_length\": 20,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_beams\": 1,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"num_labels\": 2,\n",
            "  \"num_labels_a\": 3,\n",
            "  \"num_labels_b\": 2,\n",
            "  \"num_return_sequences\": 1,\n",
            "  \"output_attentions\": false,\n",
            "  \"output_hidden_states\": false,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": null,\n",
            "  \"pooler_fc_size\": 768,\n",
            "  \"pooler_num_attention_heads\": 12,\n",
            "  \"pooler_num_fc_layers\": 3,\n",
            "  \"pooler_size_per_head\": 128,\n",
            "  \"pooler_type\": \"first_token_transform\",\n",
            "  \"pruned_heads\": {},\n",
            "  \"repetition_penalty\": 1.0,\n",
            "  \"temperature\": 1.0,\n",
            "  \"top_k\": 50,\n",
            "  \"top_p\": 1.0,\n",
            "  \"torchscript\": false,\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_bfloat16\": false,\n",
            "  \"vocab_size\": 105879\n",
            "}\n",
            "\n",
            "03/14/2020 11:16:38 - INFO - transformers.modeling_utils -   loading weights file trained-model/pytorch_model.bin\n",
            "03/14/2020 11:16:43 - INFO - __main__ -   Creating features from dataset file at ./\n",
            "03/14/2020 11:16:43 - INFO - trac_dataloader -   Writing example 0 of 1200\n",
            "03/14/2020 11:16:44 - INFO - __main__ -   Saving features into cached file ./cached_test_bert-base-multilingual-uncased_512_trac\n",
            "03/14/2020 11:16:45 - INFO - __main__ -   ***** Running evaluation  *****\n",
            "03/14/2020 11:16:45 - INFO - __main__ -     Num examples = 1200\n",
            "03/14/2020 11:16:45 - INFO - __main__ -     Batch size = 16\n",
            "Evaluating: 100% 75/75 [00:20<00:00,  3.68it/s]\n",
            "03/14/2020 11:17:05 - INFO - __main__ -   trained\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8HhX2pm4-tMd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import output\n",
        "i = 0\n",
        "while i < 100:\n",
        "    output.eval_js('new Audio(\"https://upload.wikimedia.org/wikipedia/commons/0/05/Beep-09.ogg\").play()')\n",
        "    i += 1"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}